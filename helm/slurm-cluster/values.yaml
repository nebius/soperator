clusterName: "slurm1"
# Additional annotations for the cluster
annotations: {}
# Whether to gracefully stop the cluster. Setting it to false after cluster has been paused starts the cluster back
pause: false
# K8s node filters used in Slurm node specifications. Define which nodes should be used to schedule pods to
k8sNodeFilters:
  - name: gpu
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "nebius.com/node-group-id"
                  operator: In
                  values:
                    - "node-group-id-here"
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
  - name: no-gpu
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "nebius.com/node-group-id"
                  operator: In
                  values:
                    - "node-group-id-here"
# Sources for the volumes used in Slurm node specifications
volumeSources:
  - name: controller-spool
    persistentVolumeClaim:
      claimName: "controller-spool-pvc"
      readOnly: false
  - name: jail
    persistentVolumeClaim:
      claimName: "jail-pvc"
      readOnly: false
#  - name: jail-snapshot
#    persistentVolumeClaim:
#      claimName: "jail-snapshot-pvc"
#      readOnly: true
#  - name: mlperf-sd
#    persistentVolumeClaim:
#      claimName: "jail-submount-mlperf-sd-pvc"
#      readOnly: false

# Secret references needed for Slurm cluster operation
secrets:
  # Secret reference required for login sshd. If secret name empty - operator generate own secret with keys
  sshdKeysName: ""
# Job performing initial jail file system population
populateJail:
  # Name of the k8s node filter
  k8sNodeFilterName: "gpu"
  # Configuration of the volume containing the custom initial jail content (the default content is used if not set)
  jailSnapshotVolume: null
  #  jailSnapshotVolume:
  #    volumeSourceName: "jail-snapshot"
  overwrite: false
ncclSettings:
  # TopologyType define type of NCCL GPU topology Enum: H100 GPU cluster, auto, custom
  topologyType: "auto"
  # TopologyData defines NCCL GPU topology
  topologyData: ""
# Periodic checks. e.g. GPU health
periodicChecks:
  # NCCL test benchmark
  ncclBenchmark:
    # Whether to enable the benchmark
    enabled: true
    # CronJob schedule. By default, runs every 3 hours
    schedule: "0 */3 * * *"
    # CronJob timeout in seconds. By default, equals to 30 min
    activeDeadlineSeconds: 1800
    # Number of successful finished jobs to retain
    successfulJobsHistoryLimit: 3
    # Number of failed finished jobs to retain
    failedJobsHistoryLimit: 3
    # NCCL test settings
    ncclArguments:
      # Minimum memory size to start NCCL with
      minBytes: "512Mb"
      # Maximum memory size to finish NCCL with
      maxBytes: "8Gb"
      # Multiplication factor between two sequential memory sizes
      stepFactor: "2"
      # NCCL timeout in its special format. By default, 20 minutes
      timeout: "20:00"
      # Threshold for benchmark result that must be guaranteed. CronJob will fail if the result is less than the threshold
      thresholdMoreThan: "400"
      # UseInfiniband defines using NCCL_P2P_DISABLE=1 NCCL_SHM_DISABLE=1 NCCL_ALGO=Ring env variables for test
      useInfiniband: false
    # Actions performed on benchmark failure
    failureActions:
      # Whether to drain Slurm node in case of benchmark failure
      setSlurmNodeDrainState: true
    # Name of the k8s node filter
    k8sNodeFilterName: "no-gpu"
slurmNodes:
  accounting:
    enabled: true
    k8sNodeFilterName: "no-gpu"
    slurmConfig: {}
      # accountingStorageTRES: gres/gpu,license/iop1
      # accountingStoreFlags: job_comment,job_env,job_extra,job_script,no_stdio
      # acctGatherInterconnectType: "acct_gather_interconnect/ofed"
      # acctGatherFilesystemType: acct_gather_filesystem/lustre
      # jobAcctGatherType: jobacct_gather/cgroup
      # jobAcctGatherFrequency: 30
      # priorityWeightAge: 1
      # priorityWeightFairshare: 1
      # priorityWeightTRES: 1
    slurmdbdConfig: {}
      # archiveEvents: "yes"
      # archiveJobs: "yes"
      # archiveSteps: "yes"
      # archiveSuspend: "yes"
      # archiveResv: "yes"
      # archiveUsage: "yes"
      # archiveTXN: "yes"
      # debugLevel: "info"
      # tcpTimeout: "120"
      # purgeEventAfter: "1month"
      # purgeJobAfter: "1month"
      # purgeStepAfter: "1month"
      # purgeSuspendAfter: "12month"
      # purgeResvAfter: "1month"
      # purgeUsageAfter: "1month"
      # debugFlags: "DB_ARCHIVE"
    slurmdbd:
      port: 6819
      resources:
        cpu: "1000m"
        memory: "3Gi"
        ephemeralStorage: "10Gi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
    externalDB:
      enabled: false
      # host: ""
      # port: 3306
      # user: ""
      # passwordSecretKeyRef:
      #   name: ""
      #   key: ""
    mariadbOperator:
      enabled: false
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
      replicas: 1
      replication: {}
      # enabled: false
      # primary: {}
      # probesEnabled: false
      # replica: {}
      # syncBinlog: false
      storage: {}
      # ephemeral: false
      # resizeInUseVolumes: false
      # size: {}
      # storageClassName: ""
      # volumeClaimTemplate: ""
      # waitForVolumeResize: false
      podSecurityContext: {}
      # fsGroup: 2000
      # runAsUser: 1000
      # runAsGroup: 3000
      # runAsNonRoot: true
      # supplementalGroups: [1000]
      securityContext: {}
      # runAsUser: 1000
      # runAsGroup: 3000
      # runAsNonRoot: true
      # capabilities:
      #   add: ["NET_ADMIN", "SYS_TIME"]
      metrics:
        enabled: false
        # username: ""
        # serviceMonitor: {}
        # jobLabel: ""
        # interval: ""
        # scrapeTimeout: ""
        # exporter: {}
        # passwordSecretKeyRef: {}
        # name: ""
        # key: ""
      # affinity: {}
      # args: []
      # env: []
      # envFrom: []
      # image: ""
      # imagePullPolicy: ""
      # imagePullSecrets: []
      # initContainers: []
      # livenessProbe: {}
      # nodeSelector: {}
      # podMetadata: {}
      # securityContext: {}
      # serviceAccountName: ""
      # sidecarContainers: []
      # tolerations: []
      # resources: {}
      # volumeMounts: []
      # volumes: []
      # resources: {}
      #   limits:
      #     cpu: "100m"
      #     memory: "128Mi"
      #   requests:
      #     cpu: "50m"
      #     memory: "64Mi"
  controller:
    size: 2
    k8sNodeFilterName: "no-gpu"
    slurmctld:
      port: 6817
      resources:
        cpu: "1000m"
        memory: "3Gi"
        ephemeralStorage: "20Gi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
    volumes:
      spool:
        volumeSourceName: "controller-spool"
      jail:
        volumeSourceName: "jail"
  worker:
    size: 2
    k8sNodeFilterName: "gpu"
    cgroupVersion: v2
    slurmd:
      port: 6818
      resources:
        cpu: "156000m"
        memory: "1220Gi"
        ephemeralStorage: "55Gi"
        gpu: 8
    munge:
      resources:
        cpu: "2000m"
        memory: "4Gi"
        ephemeralStorage: "5Gi"
    volumes:
      spool:
        volumeClaimTemplateSpec:
          storageClassName: "nebius-network-ssd"
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: "128Gi"
      jail:
        volumeSourceName: "jail"
      jailSubMounts: []
      #      jailSubMounts:
      #        - name: "mlcommons-sd-bench-data"
      #          mountPath: "/mlperf-sd"
      #          volumeSourceName: "mlperf-sd"
  login:
    size: 2
    k8sNodeFilterName: "no-gpu"
    sshd:
      port: 22
      resources:
        cpu: "3000m"
        memory: "9Gi"
        ephemeralStorage: "30Gi"
    # Authorized keys required for SSH connection to Slurm login nodes
    sshRootPublicKeys:
      - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKzxkjzPQ4EyZSjan4MLGFSA18idpZicoKW7Hfff username1"
      - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICL8scMKnwu+Y9S6XDACacZ54+qu+YRo2y4Ieddd username2"
    # Either `LoadBalancer` or `NodePort`
    sshdServiceType: "NodePort"
    # Required in case of sshdServiceType == `LoadBalancer`
    sshdServiceLoadBalancerIP: <your-ip-here>
    # Required in case of sshdServiceType == `NodePort`
    sshdServiceNodePort: 30022
    munge:
      resources:
        cpu: "500m"
        memory: "500Mi"
        ephemeralStorage: "5Gi"
    volumes:
      jail:
        volumeSourceName: "jail"
      jailSubMounts: []
      #      jailSubMounts:
      #        - name: "mlcommons-sd-bench-data"
      #          mountPath: "/mlperf-sd"
      #          volumeSourceName: "mlperf-sd"
  exporter:
    enabled: true
    size: 1
    k8sNodeFilterName: "no-gpu"
    exporter:
      resources:
        cpu: "250m"
        memory: "256Mi"
        ephemeralStorage: "500Mi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
    volumes:
      jail:
        volumeSourceName: "jail"
telemetry: {}
# jobsTelemetry:
#   otelCollectorHttpHost: vmsingle-slurm.monitoring-system.svc.cluster.local
#   otelCollectorPath: /opentelemetry/api/v1/push
#   otelCollectorPort: 8429
#   sendJobsEvents: true
#   sendOtelMetrics: true
# openTelemetryCollector:
#   enabled: true
#   replicasOtelCollector: 1
#   otelCollectorPort: 8429

images:
  slurmctld: "ghcr.io/nebius/soperator/controller_slurmctld:1.14.3-jammy-slurm24.05.2"
  slurmd: "ghcr.io/nebius/soperator/worker_slurmd:1.14.3-jammy-slurm24.05.2"
  sshd: "ghcr.io/nebius/soperator/login_sshd:1.14.3-jammy-slurm24.05.2"
  munge: "ghcr.io/nebius/soperator/munge:1.14.3-jammy-slurm24.05.2"
  populateJail: "ghcr.io/nebius/soperator/populate_jail:1.14.3-jammy-slurm24.05.2"
  ncclBenchmark: "ghcr.io/nebius/soperator/nccl_benchmark:1.14.3-jammy-slurm24.05.2"
  slurmdbd: "ghcr.io/nebius/soperator/controller_slurmdbd:1.14.3-jammy-slurm24.05.2"
  exporter: "ghcr.io/nebius/soperator/exporter:1.14.3-jammy-slurm24.05.2"
  mariaDB: "docker-registry1.mariadb.com/library/mariadb:11.4.3"
