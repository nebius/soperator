clusterName: "slurm1"
# Additional annotations for the cluster
annotations: {}
# Whether to gracefully stop the cluster. Setting it to false after cluster has been paused starts the cluster back
pause: false
# K8s node filters used in Slurm node specifications. Define which nodes should be used to schedule pods to
k8sNodeFilters:
  - name: gpu
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "nebius.com/node-group-id"
                  operator: In
                  values:
                    - "f2mpmlsb2me7caebvaje"
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
  - name: no-gpu
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "nebius.com/node-group-id"
                  operator: In
                  values:
                    - "f2misrg1i9vtg1mtcqg3"
# Sources for the volumes used in Slurm node specifications
volumeSources:
  - name: controller-spool
    persistentVolumeClaim:
      claimName: "controller-spool-pvc"
      readOnly: false
  - name: jail
    persistentVolumeClaim:
      claimName: "jail-pvc"
      readOnly: false
#  - name: jail-snapshot
#    persistentVolumeClaim:
#      claimName: "jail-snapshot-pvc"
#      readOnly: true
#  - name: mlperf-sd
#    persistentVolumeClaim:
#      claimName: "mlperf-sd-pvc"
#      readOnly: false

# Secret references needed for Slurm cluster operation
secrets:
  # Secret reference required for login sshd. If secret name empty - operator generate own secret with keys
  sshdKeysName: ""
# Job performing initial jail file system population
populateJail:
  # Name of the k8s node filter
  k8sNodeFilterName: "gpu"
  # Configuration of the volume containing the custom initial jail content (the default content is used if not set)
  jailSnapshotVolume: null
  #  jailSnapshotVolume:
  #    volumeSourceName: "jail-snapshot"
  overwrite: false
ncclSettings:
  # TopologyType define type of NCCL GPU topology Enum: H100 GPU cluster, auto, custom
  topologyType: "auto"
  # TopologyData defines NCCL GPU topology
  topologyData: ""
# Periodic checks. e.g. GPU health
periodicChecks:
  # NCCL test benchmark
  ncclBenchmark:
    # Whether to enable the benchmark
    enabled: true
    # CronJob schedule. By default, runs every 3 hours
    schedule: "0 */3 * * *"
    # CronJob timeout in seconds. By default, equals to 30 min
    activeDeadlineSeconds: 1800
    # Number of successful finished jobs to retain
    successfulJobsHistoryLimit: 3
    # Number of failed finished jobs to retain
    failedJobsHistoryLimit: 3
    # NCCL test settings
    ncclArguments:
      # Minimum memory size to start NCCL with
      minBytes: "512Mb"
      # Maximum memory size to finish NCCL with
      maxBytes: "8Gb"
      # Multiplication factor between two sequential memory sizes
      stepFactor: "2"
      # NCCL timeout in its special format. By default, 20 minutes
      timeout: "20:00"
      # Threshold for benchmark result that must be guaranteed. CronJob will fail if the result is less than the threshold
      thresholdMoreThan: "400"
      # UseInfiniband defines using NCCL_P2P_DISABLE=1 NCCL_SHM_DISABLE=1 NCCL_ALGO=Ring env variables for test
      useInfiniband: false
    # Actions performed on benchmark failure
    failureActions:
      # Whether to drain Slurm node in case of benchmark failure
      setSlurmNodeDrainState: true
    # Name of the k8s node filter
    k8sNodeFilterName: "no-gpu"
slurmNodes:
  controller:
    size: 2
    k8sNodeFilterName: "no-gpu"
    slurmctld:
      port: 6817
      resources:
        cpu: "1000m"
        memory: "3Gi"
        ephemeralStorage: "20Gi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
    volumes:
      spool:
        volumeSourceName: "controller-spool"
      jail:
        volumeSourceName: "jail"
  worker:
    size: 2
    k8sNodeFilterName: "gpu"
    slurmd:
      port: 6818
      resources:
        cpu: "156000m"
        memory: "1220Gi"
        ephemeralStorage: "55Gi"
        gpu: 8
    munge:
      resources:
        cpu: "2000m"
        memory: "4Gi"
        ephemeralStorage: "5Gi"
    volumes:
      spool:
        volumeClaimTemplateSpec:
          storageClassName: "nebius-network-ssd"
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: "128Gi"
      jail:
        volumeSourceName: "jail"
      jailSubMounts: []
      #      jailSubMounts:
      #        - name: "mlcommons-sd-bench-data"
      #          mountPath: "/mlperf-sd"
      #          volumeSourceName: "mlperf-sd"
  login:
    size: 2
    k8sNodeFilterName: "no-gpu"
    sshd:
      port: 22
      resources:
        cpu: "3000m"
        memory: "9Gi"
        ephemeralStorage: "30Gi"
    # Authorized keys required for SSH connection to Slurm login nodes
    sshRootPublicKeys:
      - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKzxkjzPQ4EyZSjan4MLGFSA18idpZicoKW7Hfff username1"
      - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICL8scMKnwu+Y9S6XDACacZ54+qu+YRo2y4Ieddd username2"
    sshdServiceType: "LoadBalancer"
    sshdServiceLoadBalancerIP: "195.242.16.24"
    munge:
      resources:
        cpu: "500m"
        memory: "500Mi"
        ephemeralStorage: "5Gi"
    volumes:
      jail:
        volumeSourceName: "jail"
      jailSubMounts: []
#      jailSubMounts:
#        - name: "mlcommons-sd-bench-data"
#          mountPath: "/mlperf-sd"
#          volumeSourceName: "mlperf-sd"

images:
  slurmctld: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/controller_slurmctld:1.8.0-jammy-slurm24.05.2"
  slurmd: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/worker_slurmd:1.8.0-jammy-slurm24.05.2"
  sshd: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/login_sshd:1.8.0-jammy-slurm24.05.2"
  munge: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/munge:1.8.0-jammy-slurm24.05.2"
  populateJail: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/populate_jail:1.8.0-jammy-slurm24.05.2"
  ncclBenchmark: cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/nccl_benchmark:1.8.0-jammy-slurm24.05.2
