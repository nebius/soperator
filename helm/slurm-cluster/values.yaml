clusterName: "slurm1"
# Additional annotations for the cluster
annotations: {}
# Whether to gracefully stop the cluster. Setting it to false after cluster has been paused starts the cluster back
pause: false
# K8s node filters used in Slurm node specifications. Define which nodes should be used to schedule pods to
k8sNodeFilters:
  - name: gpu
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "nebius.com/node-group-id"
                  operator: In
                  values:
                    - "node-group-id-here"
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
  - name: no-gpu
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: "nebius.com/node-group-id"
                  operator: In
                  values:
                    - "node-group-id-here"
# Sources for the volumes used in Slurm node specifications
volumeSources:
  - name: controller-spool
    persistentVolumeClaim:
      claimName: "controller-spool-pvc"
      readOnly: false
  - name: jail
    persistentVolumeClaim:
      claimName: "jail-pvc"
      readOnly: false
#  - name: jail-snapshot
#    persistentVolumeClaim:
#      claimName: "jail-snapshot-pvc"
#      readOnly: true
#  - name: mlperf-sd
#    persistentVolumeClaim:
#      claimName: "jail-submount-mlperf-sd-pvc"
#      readOnly: false

# Secret references needed for Slurm cluster operation
secrets:
  # Secret reference required for login sshd. If secret name empty - operator generate own secret with keys
  sshdKeysName: ""
# Job performing initial jail file system population
populateJail:
  # Name of the k8s node filter
  k8sNodeFilterName: "gpu"
  # Configuration of the volume containing the custom initial jail content (the default content is used if not set)
  jailSnapshotVolume: null
  #  jailSnapshotVolume:
  #    volumeSourceName: "jail-snapshot"
  overwrite: false
ncclSettings:
  # TopologyType define type of NCCL GPU topology Enum: H100 GPU cluster, auto, custom
  topologyType: "auto"
  # TopologyData defines NCCL GPU topology
  topologyData: ""
# Periodic checks. e.g. GPU health
periodicChecks:
  # NCCL test benchmark
  ncclBenchmark:
    # Whether to enable the benchmark
    enabled: true
    # CronJob schedule. By default, runs every 3 hours
    schedule: "0 */3 * * *"
    # CronJob timeout in seconds. By default, equals to 30 min
    activeDeadlineSeconds: 1800
    # Number of successful finished jobs to retain
    successfulJobsHistoryLimit: 3
    # Number of failed finished jobs to retain
    failedJobsHistoryLimit: 3
    # NCCL test settings
    ncclArguments:
      # Minimum memory size to start NCCL with
      minBytes: "512Mb"
      # Maximum memory size to finish NCCL with
      maxBytes: "8Gb"
      # Multiplication factor between two sequential memory sizes
      stepFactor: "2"
      # NCCL timeout in its special format. By default, 20 minutes
      timeout: "20:00"
      # Threshold for benchmark result that must be guaranteed. CronJob will fail if the result is less than the threshold
      thresholdMoreThan: "400"
      # UseInfiniband defines using NCCL_P2P_DISABLE=1 NCCL_SHM_DISABLE=1 NCCL_ALGO=Ring env variables for test
      useInfiniband: false
    # Actions performed on benchmark failure
    failureActions:
      # Whether to drain Slurm node in case of benchmark failure
      setSlurmNodeDrainState: true
    # Name of the k8s node filter
    k8sNodeFilterName: "no-gpu"
slurmNodes:
  accounting:
    enabled: false
    externalDB:
      enabled: false
      # host: ""
      # port: 3306
      # user: ""
      # passwordSecretKeyRef:
      #   name: ""
      #   key: ""
    k8sNodeFilterName: "no-gpu"
    slurmdbd:
      port: 6819
      resources:
        cpu: "1000m"
        memory: "3Gi"
        ephemeralStorage: "10Gi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
  controller:
    size: 2
    k8sNodeFilterName: "no-gpu"
    slurmctld:
      port: 6817
      resources:
        cpu: "1000m"
        memory: "3Gi"
        ephemeralStorage: "20Gi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
    volumes:
      spool:
        volumeSourceName: "controller-spool"
      jail:
        volumeSourceName: "jail"
  worker:
    size: 2
    k8sNodeFilterName: "gpu"
    cgroupVersion: v2
    slurmd:
      port: 6818
      resources:
        cpu: "156000m"
        memory: "1220Gi"
        ephemeralStorage: "55Gi"
        gpu: 8
    munge:
      resources:
        cpu: "2000m"
        memory: "4Gi"
        ephemeralStorage: "5Gi"
    volumes:
      spool:
        volumeClaimTemplateSpec:
          storageClassName: "nebius-network-ssd"
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: "128Gi"
      jail:
        volumeSourceName: "jail"
      jailSubMounts: []
      #      jailSubMounts:
      #        - name: "mlcommons-sd-bench-data"
      #          mountPath: "/mlperf-sd"
      #          volumeSourceName: "mlperf-sd"
  login:
    size: 2
    k8sNodeFilterName: "no-gpu"
    sshd:
      port: 22
      resources:
        cpu: "3000m"
        memory: "9Gi"
        ephemeralStorage: "30Gi"
    # Authorized keys required for SSH connection to Slurm login nodes
    sshRootPublicKeys:
      - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKzxkjzPQ4EyZSjan4MLGFSA18idpZicoKW7Hfff username1"
      - "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICL8scMKnwu+Y9S6XDACacZ54+qu+YRo2y4Ieddd username2"
    # Either `LoadBalancer` or `NodePort`
    sshdServiceType: "NodePort"
    # Required in case of sshdServiceType == `LoadBalancer`
    sshdServiceLoadBalancerIP: <your-ip-here>
    # Required in case of sshdServiceType == `NodePort`
    sshdServiceNodePort: 30022
    munge:
      resources:
        cpu: "500m"
        memory: "500Mi"
        ephemeralStorage: "5Gi"
    volumes:
      jail:
        volumeSourceName: "jail"
      jailSubMounts: []
      #      jailSubMounts:
      #        - name: "mlcommons-sd-bench-data"
      #          mountPath: "/mlperf-sd"
      #          volumeSourceName: "mlperf-sd"
  exporter:
    enabled: true
    size: 1
    k8sNodeFilterName: "no-gpu"
    exporter:
      resources:
        cpu: "250m"
        memory: "256Mi"
        ephemeralStorage: "500Mi"
    munge:
      resources:
        cpu: "1000m"
        memory: "1Gi"
        ephemeralStorage: "5Gi"
    volumes:
      jail:
        volumeSourceName: "jail"
telemetry: {}
# jobsTelemetry:
#   otelCollectorHttpHost: vmsingle-slurm.monitoring-system.svc.cluster.local
#   otelCollectorPath: /opentelemetry/api/v1/push
#   otelCollectorPort: 8429
#   sendJobsEvents: true
#   sendOtelMetrics: true
# openTelemetryCollector:
#   enabled: true
#   replicasOtelCollector: 1
#   otelCollectorPort: 8429

images:
  slurmctld: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/controller_slurmctld:1.13.5-jammy-slurm24.05.2"
  slurmd: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/worker_slurmd:1.13.5-jammy-slurm24.05.2"
  sshd: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/login_sshd:1.13.5-jammy-slurm24.05.2"
  munge: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/munge:1.13.5-jammy-slurm24.05.2"
  populateJail: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/populate_jail:1.13.5-jammy-slurm24.05.2"
  ncclBenchmark: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/nccl_benchmark:1.13.5-jammy-slurm24.05.2"
  slurmdbd: cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/controller_slurmdbd:1.13.5-jammy-slurm24.05.2
  exporter: "cr.nemax.nebius.cloud/crnlu9nio77sg3p8n5bi/exporter:1.13.5-jammy-slurm24.05.2"
