{{- if and .Values.observability.enabled
            .Values.observability.opentelemetry.enabled
            .Values.observability.opentelemetry.logs.values.jailLogs.enabled }}
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: {{ include "soperator-fluxcd.fullname" . }}-opentelemetry-collector-jail-logs
  labels:
  {{- include "soperator-fluxcd.labels" . | nindent 4 }}
spec:
  chart:
    spec:
      chart: opentelemetry-collector
      interval: {{ .Values.observability.opentelemetry.logs.interval }}
      sourceRef:
        kind: HelmRepository
        name: {{ include "soperator-fluxcd.fullname" . }}-opentelemetry-collector
      version: {{ .Values.observability.opentelemetry.logs.version }}
  dependsOn:
  - name: {{ include "soperator-fluxcd.fullname" . }}-ns
  - name: {{ include "soperator-fluxcd.fullname" . }}-vm-logs
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
      remediateLastFailure: true
  interval: {{ .Values.observability.opentelemetry.logs.interval }}
  timeout: {{ .Values.observability.opentelemetry.logs.timeout }}
  releaseName: opentelemetry-collector-jail-logs
  targetNamespace: logs-system
  values:
  {{- $hasPublicEndpoint := .Values.observability.publicEndpointEnabled }}
  {{- if .Values.observability.opentelemetry.logs.overrideValues }}
    {{- toYaml .Values.observability.opentelemetry.logs.overrideValues | nindent 4 }}
  {{- else }}
    clusterRole:
      create: true
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - namespaces
        verbs:
        - get
        - watch
        - list
    config:
      receivers:
        filelog:
          include:
            - "/mnt/jail/opt/soperator-outputs/*/*.out"
          exclude:
            - "/mnt/jail/opt/soperator-outputs/health_checker_cmd_stdout/*.out"
          include_file_name: true  # Adds attributes["log.file.name"]
          include_file_path: true  # Adds attributes["log.file.path"]
          preserve_leading_whitespaces: true
          poll_interval: {{ .Values.observability.opentelemetry.logs.values.jailLogs.pollInterval | default "5s" }}
          operators:
            - id: extract_base_name_and_pod
              type: regex_parser
              parse_from: attributes["log.file.name"]
              regex: ^(?P<slurm_node_name>[^.]+)\.(?P<base_name>.+)\.out$
            - type: copy
              from: attributes.slurm_node_name
              to: resource["slurm_node_name"]
            - type: move
              from: attributes.slurm_node_name
              to: resource["k8s.pod.name"]
            - type: add
              field: resource["k8s.namespace.name"]
              value: {{ .Values.slurmCluster.namespace | default "soperator" | quote }}
            - id: extract_directory
              type: regex_parser
              parse_from: attributes["log.file.path"]
              regex: ^.*/(?P<log_directory>nccl_logs|slurm_jobs|slurm_scripts)/.*$
            - type: move
              from: attributes.log_directory
              to: resource["log_type"]
            - id: parse_nccl_log_base_name
              type: regex_parser
              parse_from: attributes.base_name
              regex: ^(?P<job_id>\d+)\.(?P<job_step_id>\d+)$
              if: resource["log_type"] == "nccl_logs"
            - type: move
              from: attributes.job_id
              to: resource["job_id"]
              if: resource["log_type"] == "nccl_logs" and attributes["job_id"] != nil
            - type: move
              from: attributes.job_step_id
              to: resource["job_step_id"]
              if: resource["log_type"] == "nccl_logs" and attributes["job_step_id"] != nil
            - id: parse_slurm_job_log_base_name
              type: regex_parser
              parse_from: attributes.base_name
              regex: ^(?P<job_name>[^.]+)\.(?P<job_id>\d+)(?:\.(?P<job_array_id>\d+))?$
              if: resource["log_type"] == "slurm_jobs"
            - type: move
              from: attributes.job_name
              to: resource["job_name"]
              if: resource["log_type"] == "slurm_jobs" and attributes["job_name"] != nil
            - type: move
              from: attributes.job_id
              to: resource["job_id"]
              if: resource["log_type"] == "slurm_jobs" and attributes["job_id"] != nil
            - type: move
              from: attributes.job_array_id
              to: resource["job_array_id"]
              if: resource["log_type"] == "slurm_jobs" and attributes["job_array_id"] != nil
            - id: parse_script_log_base_name
              type: regex_parser
              parse_from: attributes.base_name
              regex: ^(?P<script_name>[^.]+)\.(?P<script_context>[^.]+)$
              if: resource["log_type"] == "slurm_scripts"
            - type: move
              from: attributes.script_name
              to: resource["slurm_script_name"]
              if: resource["log_type"] == "slurm_scripts" and attributes["script_name"] != nil
            - type: move
              from: attributes.script_context
              to: resource["slurm_script_context"]
              if: resource["log_type"] == "slurm_scripts" and attributes["script_context"] != nil
            - type: remove
              field: attributes.base_name
          retry_on_failure:
            enabled: true
          start_at: end  # Start reading from the end of the file on startup.
          storage: file_storage  # Storage for the state (file offsets).
        filelog/health_checker:
          include:
            - "/mnt/jail/opt/soperator-outputs/health_checker_cmd_stdout/*.out"
          include_file_name: true  # Adds attributes["log.file.name"]
          include_file_path: true  # Adds attributes["log.file.path"]
          preserve_leading_whitespaces: true
          poll_interval: {{ .Values.observability.opentelemetry.logs.values.jailLogs.pollInterval | default "5s" }}
          operators:
            # Extract check_name and run_id
            - id: extract_check_name_and_run_id
              type: regex_parser
              parse_from: attributes["log.file.name"]
              regex: ^(?P<check_name>.+)\.(?P<run_id>[0-9a-fA-F-]+)\.out$
            - type: copy
              from: attributes.check_name
              to: resource["health_checker.check_name"]
            - type: copy
              from: attributes.run_id
              to: resource["health_checker.run_id"]

            # Add namespace
            - type: add
              field: resource["k8s.namespace.name"]
              value: {{ .Values.slurmCluster.namespace | default "soperator" | quote }}

            # Add log_type
            - type: add
              field: resource["log_type"]
              value: "health_checker_cmd_stdout"

          retry_on_failure:
            enabled: true
          start_at: end  # Start reading from the end of the file on startup.
          storage: file_storage  # Storage for the state (file offsets).
        # Disable irrelevant receivers that might be added by valuesFrom ConfigMap:
        zipkin: null
        jaeger: null
        otlp: null
        prometheus: null
      processors:
        batch:
          send_batch_max_size: 700
          send_batch_size: 250
        k8sattributes:
          pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
          extract:
            labels:  # Copy labels for the managed solution.
            - from: pod
              key: external-o11y
              tag_name: external_o11y
            - from: namespace
              key: o11y_resource_id
              tag_name: resource_id
            - from: namespace
              key: o11y_service_provider
              tag_name: service_provider
            metadata:
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.pod.start_time
          passthrough: false
          wait_for_metadata: true  # Wait for k8s metadata on startup.
          wait_for_metadata_timeout: 30s
        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25
        transform:
          log_statements:
          - context: log
            error_mode: ignore
            statements:
            - set(attributes["cluster"], {{ .Values.observability.clusterName | quote }})
      exporters:
        otlphttp/victorialogs:
          compression: gzip
          encoding: proto
          logs_endpoint: http://vm-logs-victoria-logs-single-server:9428/insert/opentelemetry/v1/logs
          retry_on_failure:
            initial_interval: 200ms
          timeout: 5s
        {{- if $hasPublicEndpoint }}
        otlp:
          endpoint: dns:///write.logging.eu-north1.nebius.cloud.:443
          balancer_name: round_robin
          compression: snappy
          retry_on_failure:
            initial_interval: 200ms
          timeout: 5s
          headers:
            iam-container: {{ .Values.observability.logsProjectId }}
          auth:
            authenticator: bearertokenauth
        {{- end }}
      extensions:
        file_storage:
          directory: /var/lib/otelcol-jail
        health_check:
          endpoint: 0.0.0.0:13133
        {{- if $hasPublicEndpoint }}
        bearertokenauth:
          filename: "/o11ytoken/accessToken"
        {{- end }}
      service:
        extensions:
        - health_check
        - file_storage
        {{- if $hasPublicEndpoint }}
        - bearertokenauth
        {{- end }}
        pipelines:
          logs:
            receivers:
            - filelog
            - filelog/health_checker
            processors:
            - k8sattributes
            - transform
            - memory_limiter
            - batch
            exporters:
            - otlphttp/victorialogs
            {{- if $hasPublicEndpoint }}
            - otlp
            {{- end }}
          metrics: null
          traces: null
    extraEnvs:
    - name: GOMAXPROCS
      value: "1"
    extraVolumeMounts:
    {{- if $hasPublicEndpoint }}
    - mountPath: /o11ytoken
      name: o11ytoken
      readOnly: true
    {{- end }}
    - mountPath: /var/lib/otelcol-jail
      name: varlibotelcol
    - mountPath: /mnt/jail/opt/soperator-outputs
      name: soperator-outputs
      readOnly: true
    extraVolumes:
    {{- if $hasPublicEndpoint }}
    - name: o11ytoken
      secret:
        secretName: o11y-writer-sa-token
    {{- end }}
    - name: soperator-outputs
      hostPath:
        path: /mnt/jail/opt/soperator-outputs
        type: Directory
    - hostPath:
        path: /var/lib/otelcol-jail
        type: DirectoryOrCreate
      name: varlibotelcol
    image:
      pullPolicy: IfNotPresent
      repository: cr.eu-north1.nebius.cloud/observability/nebius-o11y-agent
      tag: 0.2.267
    initContainers:
    - command:
      - sh
      - -c
      - 'chown -R 10001: /var/lib/otelcol-jail'
      image: cr.eu-north1.nebius.cloud/soperator/busybox:latest
      name: init-fs
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      volumeMounts:
      - mountPath: /var/lib/otelcol-jail
        name: varlibotelcol
    mode: deployment  # Single deployment on system nodes instead of DaemonSet on all workers
    replicaCount: 1   # Only one replica to avoid file lock conflicts on shared storage and prevent log duplication
    rollout:
      strategy: Recreate  # Ensure old pod terminates before new one starts to maintain max one pod running
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: slurm.nebius.ai/nodeset
                  operator: In
                  values: ["system"]
    ports:  # Disable all network receivers since this collector only reads files
      jaeger-compact:
        enabled: false
      jaeger-grpc:
        enabled: false
      jaeger-thrift:
        enabled: false
      otlp:
        enabled: false
      otlp-http:
        enabled: false
      zipkin:
        enabled: false
    securityContext:
      runAsGroup: 0
      runAsUser: 10001
    service:  # No external access needed - this collector only reads files and sends to Victoria Logs
      enabled: false
    serviceAccount:
      create: true
    useGOMEMLIMIT: {{ .Values.observability.opentelemetry.logs.values.useGOMEMLIMIT | default true }}
    {{- if .Values.observability.opentelemetry.logs.values.jailLogs.resources }}
    resources: {{- toYaml .Values.observability.opentelemetry.logs.values.jailLogs.resources | nindent 6 }}
    {{- end }}
  {{- end }}
  valuesFrom:
  - kind: ConfigMap
    name: terraform-opentelemetry-collector-jail-logs
    optional: true
    valuesKey: values.yaml
  - kind: ConfigMap
    name: opentelemetry-collector-jail-logs
    optional: true
    valuesKey: values.yaml
{{- end }}
