{{- if and .Values.observability.enabled
            .Values.observability.opentelemetry.enabled
            .Values.observability.opentelemetry.logs.values.hcOutputEnabled
            .Values.observability.opentelemetry.logs.values.jailLogs.enabled }}
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: {{ include "soperator-fluxcd.fullname" . }}-opentelemetry-collector-jail-logs
  labels:
  {{- include "soperator-fluxcd.labels" . | nindent 4 }}
spec:
  chart:
    spec:
      chart: opentelemetry-collector
      interval: {{ .Values.observability.opentelemetry.logs.interval }}
      sourceRef:
        kind: HelmRepository
        name: {{ include "soperator-fluxcd.fullname" . }}-opentelemetry-collector
      version: {{ .Values.observability.opentelemetry.logs.version }}
  dependsOn:
  - name: {{ include "soperator-fluxcd.fullname" . }}-ns
  - name: {{ include "soperator-fluxcd.fullname" . }}-vm-logs
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
      remediateLastFailure: true
  interval: {{ .Values.observability.opentelemetry.logs.interval }}
  timeout: {{ .Values.observability.opentelemetry.logs.timeout }}
  releaseName: opentelemetry-collector-jail-logs
  targetNamespace: logs-system
  values:
  {{- $hasPublicEndpoint := .Values.observability.publicEndpointEnabled }}
  {{- if .Values.observability.opentelemetry.logs.overrideValues }}
    {{- toYaml .Values.observability.opentelemetry.logs.overrideValues | nindent 4 }}
  {{- else }}
    clusterRole:
      create: true
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - namespaces
        verbs:
        - get
        - watch
        - list
    config:
      exporters:
        otlphttp/victoriametrics:
          compression: gzip
          encoding: proto
          logs_endpoint: http://vm-logs-victoria-logs-single-server:9428/insert/opentelemetry/v1/logs
          retry_on_failure:
            initial_interval: 200ms
          timeout: 5s
        {{- if $hasPublicEndpoint }}
        otlp:
          endpoint: dns:///write.logging.eu-north1.nebius.cloud.:443
          balancer_name: round_robin
          compression: snappy
          retry_on_failure:
            initial_interval: 200ms
          timeout: 5s
          headers:
            iam-container: project-e00h61cxzwnf6zksvdn77
          auth:
            authenticator: bearertokenauth
        {{- end }}
      extensions:
        k8s_observer:
          node: ${env:K8S_NODE_NAME}
        file_storage:
          directory: /var/lib/otelcol-jail
        health_check:
          endpoint: 0.0.0.0:13133
        {{- if $hasPublicEndpoint }}
        bearertokenauth:
          filename: "/o11ytoken/accessToken"
        {{- end }}
      processors:
        batch:
          send_batch_max_size: 700
          send_batch_size: 250
        k8sattributes:
          extract:
            labels:
            - from: pod
              key: external-o11y
              tag_name: external_o11y
            - from: namespace
              key: o11y_resource_id
              tag_name: resource_id
            - from: namespace
              key: o11y_service_provider
              tag_name: service_provider
            metadata:
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.pod.start_time
          filter:
            node_from_env_var: K8S_NODE_NAME
          passthrough: false
          pod_association:
          - sources:
            - from: resource_attribute
              name: k8s.pod.ip
          - sources:
            - from: resource_attribute
              name: k8s.pod.uid
          - sources:
            - from: connection
          wait_for_metadata: true
          wait_for_metadata_timeout: 30s
        memory_limiter:
          check_interval: 5s
          limit_percentage: 80
          spike_limit_percentage: 25
        transform:
          log_statements:
          - context: log
            error_mode: ignore
            statements:
            - set(attributes["cluster"], "${env:CLUSTER_NAME}")
            - set(attributes["k8s.node.name"], "${env:K8S_NODE_NAME}")
      receivers:
        filelog:
          include:
            - "/mnt/jail/opt/soperator-outputs/*/*.out"
          include_file_name: true
          include_file_path: true
          preserve_leading_whitespaces: true
          poll_interval: {{ .Values.observability.opentelemetry.logs.values.jailLogs.pollInterval | default "5s" }}
          operators:
            - id: extract_content_from_filename
              type: regex_parser
              parse_from: attributes["log.file.name"]
              regex: ^(?P<content>.+)\.out$
            - id: extract_directory
              type: regex_parser
              parse_from: attributes["log.file.path"]
              regex: ^.*/(?P<log_directory>nccl_logs|slurm_jobs|slurm_scripts)/.*$
            - type: move
              from: attributes.log_directory
              to: resource["log_type"]
            - id: parse_nccl_content
              type: regex_parser
              parse_from: attributes.content
              regex: ^(?P<node_name>[^.]+)\.(?P<job_id>\d+)\.(?P<job_step_id>\d+)$
              if: resource["log_type"] == "nccl_logs"
            - type: move
              from: attributes.job_id
              to: resource["job_id"]
              if: resource["log_type"] == "nccl_logs" and attributes["job_id"] != nil
            - type: move
              from: attributes.job_step_id
              to: resource["job_step_id"]
              if: resource["log_type"] == "nccl_logs" and attributes["job_step_id"] != nil
            - type: move
              from: attributes.node_name
              to: resource["node_name"]
              if: resource["log_type"] == "nccl_logs" and attributes["node_name"] != nil
            - id: parse_slurm_job_content
              type: regex_parser
              parse_from: attributes.content
              regex: ^(?P<node_name>[^.]+)\.(?P<job_name>[^.]+)\.(?P<job_id>\d+)(?:\.(?P<job_array_id>\d+))?$
              if: resource["log_type"] == "slurm_jobs"
            - type: move
              from: attributes.job_name
              to: resource["job_name"]
              if: resource["log_type"] == "slurm_jobs" and attributes["job_name"] != nil
            - type: move
              from: attributes.job_id
              to: resource["job_id"]
              if: resource["log_type"] == "slurm_jobs" and attributes["job_id"] != nil
            - type: move
              from: attributes.job_array_id
              to: resource["job_array_id"]
              if: resource["log_type"] == "slurm_jobs" and attributes["job_array_id"] != nil
            - type: move
              from: attributes.node_name
              to: resource["node_name"]
              if: resource["log_type"] == "slurm_jobs" and attributes["node_name"] != nil
            - id: parse_script_content
              type: regex_parser
              parse_from: attributes.content
              regex: ^(?P<node_name>[^.]+)\.(?P<script_name>[^.]+)\.(?P<script_context>[^.]+)$
              if: resource["log_type"] == "slurm_scripts"
            - type: move
              from: attributes.script_name
              to: resource["slurm_script_name"]
              if: resource["log_type"] == "slurm_scripts" and attributes["script_name"] != nil
            - type: move
              from: attributes.script_context
              to: resource["slurm_script_context"]
              if: resource["log_type"] == "slurm_scripts" and attributes["script_context"] != nil
            - type: move
              from: attributes.node_name
              to: resource["node_name"]
              if: resource["log_type"] == "slurm_scripts" and attributes["node_name"] != nil
            - type: remove
              field: attributes.content
          retry_on_failure:
            enabled: true
          start_at: end
          storage: file_storage
        zipkin: null
      service:
        extensions:
        - health_check
        - file_storage
        {{- if $hasPublicEndpoint }}
        - bearertokenauth
        {{- end }}
        - k8s_observer
        pipelines:
          logs:
            exporters:
            - otlphttp/victoriametrics
            {{- if $hasPublicEndpoint }}
            - otlp
            {{- end }}
            processors:
            - k8sattributes
            - transform
            - memory_limiter
            - batch
            receivers:
            - filelog
          metrics: null
          traces: null
    extraEnvs:
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: CLUSTER_NAME
      value: {{ .Values.observability.clusterName | quote }}
    - name: GOMAXPROCS
      value: "1"
    extraVolumeMounts:
    {{- if $hasPublicEndpoint }}
    - mountPath: /o11ytoken
      name: o11ytoken
      readOnly: true
    {{- end }}
    - mountPath: /var/lib/otelcol-jail
      name: varlibotelcol
    - mountPath: /mnt/jail/opt/soperator-outputs
      name: soperator-outputs
      readOnly: true
    extraVolumes:
    {{- if $hasPublicEndpoint }}
    - name: o11ytoken
      secret:
        secretName: o11y-writer-sa-token
    {{- end }}
    - hostPath:
        path: /mnt/jail/opt/soperator-outputs
        type: Directory
      name: soperator-outputs
    - hostPath:
        path: /var/lib/otelcol-jail
        type: DirectoryOrCreate
      name: varlibotelcol
    image:
      pullPolicy: IfNotPresent
      repository: cr.eu-north1.nebius.cloud/observability/nebius-o11y-agent
      tag: 0.2.267
    initContainers:
    - command:
      - sh
      - -c
      - 'chown -R 10001: /var/lib/otelcol-jail'
      image: cr.eu-north1.nebius.cloud/soperator/busybox:latest
      name: init-fs
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      volumeMounts:
      - mountPath: /var/lib/otelcol-jail
        name: varlibotelcol
    mode: daemonset
    ports:
      jaeger-compact:
        enabled: false
      jaeger-grpc:
        enabled: false
      jaeger-thrift:
        enabled: false
      otlp:
        enabled: false
      otlp-http:
        enabled: false
      zipkin:
        enabled: false
    rollout:
      rollingUpdate:
        maxUnavailable: 50%
      strategy: RollingUpdate
    securityContext:
      runAsGroup: 0
      runAsUser: 10001
    serviceAccount:
      create: true
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: slurm.nebius.ai/nodeset
              operator: NotIn
              values: ["login", "accounting", "controller", "system"]
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    - effect: NoSchedule
      key: node.cilium.io/agent-not-ready
      operator: Exists
    useGOMEMLIMIT: {{ .Values.observability.opentelemetry.logs.values.useGOMEMLIMIT | default true }}
    {{- if and .Values.observability.opentelemetry.logs.values .Values.observability.opentelemetry.logs.values.resources }}
    resources: {{- toYaml .Values.observability.opentelemetry.logs.values.resources | nindent 6 }}
    {{- end }}
  {{- end }}
  valuesFrom:
  - kind: ConfigMap
    name: terraform-opentelemetry-collector-jail-logs
    optional: true
    valuesKey: values.yaml
  - kind: ConfigMap
    name: opentelemetry-collector-jail-logs
    optional: true
    valuesKey: values.yaml
{{- end }}
