suite: test nodesets volumes configuration
templates:
  - templates/nodeset.yaml
tests:
  - it: should configure volumes correctly for GPU workers
    set:
      nodesets:
        - name: gpu-workers
          replicas: 3
          slurmd:
            image:
              repository: "test/slurm"
            resources:
              cpu: "4"
              memory: "8Gi"
            volumes:
              spool:
                volumeClaimTemplateSpec:
                  storageClassName: "nebius-network-ssd"
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: "128Gi"
              jail:
                persistentVolumeClaim:
                  claimName: "jail-pvc"
              jailSubMounts: []
              sharedMemorySize: "64Gi"
            security:
              appArmorProfile: "unconfined"
              limitsConfig: ""
          munge:
            image:
              repository: "test/munge"
            resources:
              cpu: "100m"
              memory: "128Mi"
            security:
              appArmorProfile: "unconfined"
              limitsConfig: ""
    documentIndex: 0
    asserts:
      - exists:
          path: spec.slurmd.volumes.spool.volumeClaimTemplateSpec
      - equal:
          path: spec.slurmd.volumes.spool.volumeClaimTemplateSpec.storageClassName
          value: "nebius-network-ssd"
      - exists:
          path: spec.slurmd.volumes.jail.persistentVolumeClaim
      - equal:
          path: spec.slurmd.volumes.jail.persistentVolumeClaim.claimName
          value: "jail-pvc"
      - equal:
          path: spec.slurmd.volumes.sharedMemorySize
          value: "64Gi"

  - it: should configure jailSubMounts correctly
    set:
      nodesets:
        - name: gpu-workers
          replicas: 3
          slurmd:
            image:
              repository: "test/slurm"
            resources:
              cpu: "4"
              memory: "8Gi"
            volumes:
              spool:
                emptyDir: {}
              jail:
                emptyDir: {}
              jailSubMounts:
                - name: "tmp"
                  mountPath: "/tmp"
                  subPath: "tmp"
                  readOnly: false
                  volumeSource:
                    emptyDir: {}
          munge:
            image:
              repository: "test/munge"
            resources:
              cpu: "100m"
              memory: "128Mi"
    documentIndex: 0
    asserts:
      - exists:
          path: spec.slurmd.volumes.jailSubMounts
      - isNotEmpty:
          path: spec.slurmd.volumes.jailSubMounts
      - equal:
          path: spec.slurmd.volumes.jailSubMounts[0].name
          value: "tmp"
      - equal:
          path: spec.slurmd.volumes.jailSubMounts[0].mountPath
          value: "/tmp"
      - equal:
          path: spec.slurmd.volumes.jailSubMounts[0].subPath
          value: "tmp"
      - equal:
          path: spec.slurmd.volumes.jailSubMounts[0].readOnly
          value: false
      - exists:
          path: spec.slurmd.volumes.jailSubMounts[0].volumeSource.emptyDir

  - it: should configure custom mounts correctly
    set:
      nodesets:
        - name: gpu-workers
          replicas: 3
          slurmd:
            image:
              repository: "test/slurm"
            resources:
              cpu: "4"
              memory: "8Gi"
            volumes:
              spool:
                emptyDir: { }
              jail:
                emptyDir: { }
              customVolumeMounts:
                - name: "tmp"
                  mountPath: "/tmp"
                  subPath: "tmp"
                  readOnly: false
                  volumeSource:
                    emptyDir: { }
          munge:
            image:
              repository: "test/munge"
            resources:
              cpu: "100m"
              memory: "128Mi"
    documentIndex: 0
    asserts:
      - exists:
          path: spec.slurmd.volumes.customVolumeMounts
      - isNotEmpty:
          path: spec.slurmd.volumes.customVolumeMounts
      - equal:
          path: spec.slurmd.volumes.customVolumeMounts[0].name
          value: "tmp"
      - equal:
          path: spec.slurmd.volumes.customVolumeMounts[0].mountPath
          value: "/tmp"
      - equal:
          path: spec.slurmd.volumes.customVolumeMounts[0].subPath
          value: "tmp"
      - equal:
          path: spec.slurmd.volumes.customVolumeMounts[0].readOnly
          value: false
      - exists:
          path: spec.slurmd.volumes.customVolumeMounts[0].volumeSource.emptyDir

  - it: should configure security context correctly
    set:
      nodesets:
        - name: gpu-workers
          replicas: 3
          slurmd:
            image:
              repository: "test/slurm"
            resources:
              cpu: "4"
              memory: "8Gi"
            volumes:
              spool:
                emptyDir: {}
              jail:
                emptyDir: {}
              jailSubMounts: []
            security:
              appArmorProfile: "unconfined"
              limitsConfig: ""
          munge:
            image:
              repository: "test/munge"
            resources:
              cpu: "100m"
              memory: "128Mi"
            security:
              appArmorProfile: "unconfined"
              limitsConfig: ""
    documentIndex: 0
    asserts:
      - equal:
          path: spec.slurmd.security.appArmorProfile
          value: "unconfined"
      - equal:
          path: spec.slurmd.security.limitsConfig
          value: ""
      - equal:
          path: spec.munge.security.appArmorProfile
          value: "unconfined"
      - equal:
          path: spec.munge.security.limitsConfig
          value: ""
