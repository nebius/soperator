storageClass:
  local:
    name: "slurm-local-pv"

volume:
  jail:
    name: "jail"
    size: "2Ti"
    # Whether to use 'filestore' or 'glusterfs'
    # TODO: get rid of glusterfs after moving to newbius
    type: "filestore"
    # Required in case of 'filestore' type
    filestoreDeviceName: "jail"
    # Required in case of 'glusterfs' type
    glusterfsHostName: ""
  controllerSpool:
    name: "controller-spool"
    size: "30Gi"
    filestoreDeviceName: "controller-spool"
  accounting:
    enabled: false
    name: "accounting"
    size: "128Gi"
    filestoreDeviceName: "accounting"
  jailSubMounts: []
  # jailSubMounts:
  #   - name: "mlperf-sd"
  #     size: "1500Gi"
  #     filestoreDeviceName: "mlperf-sd"

storage:
  accounting:
    matchExpressions: []
    # matchExpressions:
    #   - key: nebius.com/node-group-id
    #     operator: In
    #     values:
    #       - "<CPU Node group ID>"
    tolerations: []
    # tolerations:
    #   - key: nvidia.com/gpu
    #     operator: Exists
    #     effect: NoSchedule

  controllerSpool:
    matchExpressions: []
    # matchExpressions:
    #   - key: nebius.com/node-group-id
    #     operator: In
    #     values:
    #       - "<CPU Node group ID>"
    tolerations: []
    # tolerations:
    #   - key: nvidia.com/gpu
    #     operator: Exists
    #     effect: NoSchedule

  jail:
    matchExpressions: []
    # matchExpressions:
    #   - key: nebius.com/node-group-id
    #     operator: In
    #     values:
    #       - "<CPU Node group ID>"
    tolerations: []
    # tolerations:
    #   - key: nvidia.com/gpu
    #     operator: Exists
    #     effect: NoSchedule

nfsClientConfig:
  enabled: true
  image: "cr.eu-north1.nebius.cloud/ml-containers/neubuntu:noble-20260206131521"
  imagePullPolicy: IfNotPresent
  configPath: "/host/etc/nfsmount.conf"
  nodeSelector: {}
  affinity: {}
  tolerations:
    - effect: NoSchedule
      key: slurm.nebius.ai/nodeset
      operator: Exists
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
