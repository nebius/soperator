apiVersion: apps/v1
kind: DaemonSet
metadata:
  namespace: {{ .Release.Namespace }}
  name: {{ include "slurm-cluster-storage.volume.spool.mount" . }}
spec:
  selector:
    matchLabels:
      slurm: {{ include "slurm-cluster-storage.volume.spool.mount" . }}
  template:
    metadata:
      labels:
        slurm: {{ include "slurm-cluster-storage.volume.spool.mount" . }}
    spec:
      initContainers:
        - name: mount-controller-spool-filestore
          image: busybox
          command:
            - "/bin/sh"
            - "-c"
            - |
                mkdir -p /host/mnt/controller-spool && \
                if mountpoint -q /host/mnt/controller-spool; then \
                    echo "The node's /mnt/controller-spool is already a mount point" && \
                    umount /host/mnt/controller-spool && \
                    echo "Unmounted /mnt/controller-spool from the node"; \
                fi && \
                mount -t virtiofs -o rw,relatime controller-spool /host/mnt/controller-spool && \
                echo "Mounted filestore to the node's /mnt/controller-spool"
          securityContext:
            privileged: true
          volumeMounts:
            - name: host-mount
              mountPath: /host/mnt
              mountPropagation: Bidirectional
      containers:
        - name: sleep
          image: busybox
          command: ["/bin/sh", "-c", "echo 'Sleep infinitely' && sleep infinity"]
      volumes:
        - name: host-mount
          hostPath:
            path: /mnt
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              # Require scheduling on Non-GPU nodes
              - matchExpressions:
                - key: nebius.com/node-group-id
                  operator: In
                  values:
                    - {{ include "slurm-cluster-storage.nodeGroup.nonGpu" . }}
